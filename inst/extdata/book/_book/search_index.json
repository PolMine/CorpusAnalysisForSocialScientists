[
["index.html", "Using Corpora in Social Science Research 1 Foreword", " Using Corpora in Social Science Research Andreas Blaette 2018-01-22 1 Foreword "],
["introduction.html", "2 Introduction", " 2 Introduction "],
["corpora.html", "3 Corpora 3.1 Keeping it Simple: The Beauty and Limits of Plain Text 3.2 Metadata and Structural Annotation 3.3 Linguistic Annotation 3.4 Indexing Corpora 3.5 Licences", " 3 Corpora 3.1 Keeping it Simple: The Beauty and Limits of Plain Text 3.2 Metadata and Structural Annotation 3.2.1 XML and HTML 3.2.2 Text Encoding Initiative (TEI) 3.2.3 Other Standards 3.3 Linguistic Annotation 3.4 Indexing Corpora 3.5 Licences "],
["queries.html", "4 Queries", " 4 Queries "],
["counting.html", "5 Counting 5.1 The Art of Counting", " 5 Counting 5.1 The Art of Counting Counting is a very basic task when working with text. Counting is essentially simple: It is nothing but making statements how often a word, or words, or linguistic patterns occurr in a corpus, or a subcorpus. However, a social scientist counting words or linguistic patterns should see counting is measuring. Counts are stable and objective, the reliability and intersubjectivity of the measurement are minor problems. Words and linguistic patterns can be observed directly, many issues associated with measuring latent variables do not arise. But counting is not trivial. Without conscious reflection, what is actually measured by counting lexical items, without an awareness that counting is measuring and needs to meet standards of validity, a simple move “from words to numbers” (…) can violate fundamental concerns of social science. Counting without methodological reflection will lead to flawed and bad science. If done well, counting can be powerful instruments. Corpora offer efficient ways to extract information from vast amounts of text efficiently. In particular, when we move from counts of patterns in one subcorpus to the dispersion of counts in one or two dimensions, the basis for comparative statements that involve a synchronic or diachronic dimension can be generated quickly. Sometimes, simply counting single words may do. Often, we may want to count more complex linguistic patterns. The following sections will discuss the move from counts to dispersions. The corpus used for the examples is the GermaParl corpus. Before moving on, load the polmineR library, and activate the corpus. library(polmineR) use(&quot;GermaParl&quot;) ## ... resetting CORPUS_REGISTRY environment variable: ## ... setting registry: /Library/Frameworks/R.framework/Versions/3.4/Resources/library/GermaParl/extdata/cwb/registry ## ... unloading rcqp library ## ... reloading rcqp library ## ... ... status: OK 5.1.1 Words The method in the polmineR-package to perform counts, it is not necessarily a surprise, is called ‘count’. It can be used for a variety of scenarios. To learn about the uses of the count-method, call the documentation (i.e. the help page) for the method. ?count Count can be applied to different objects. We start by count(&quot;GERMAPARL&quot;, query = &quot;Zuwanderer&quot;) ## query count freq ## 1: Zuwanderer 540 5.345809e-06 auslaender &lt;- count(&quot;GERMAPARL&quot;, query = &quot;Ausländer&quot;)[[&quot;count&quot;]] zuwanderer &lt;- count(&quot;GERMAPARL&quot;, query = &quot;Zuwanderer&quot;)[[&quot;count&quot;]] einwanderer &lt;- count(&quot;GERMAPARL&quot;, query = &quot;Einwanderer&quot;)[[&quot;count&quot;]] migranten &lt;- count(&quot;GERMAPARL&quot;, query = &quot;Migranten&quot;)[[&quot;count&quot;]] gastarbeiter &lt;- count(&quot;GERMAPARL&quot;, query = &quot;Gastarbeiter&quot;)[[&quot;count&quot;]] barplot( c(auslaender, zuwanderer, einwanderer, migranten), names.arg = c(&quot;Ausländer&quot;, &quot;Zuwanderer&quot;, &quot;Einwanderer&quot;, &quot;Migranten&quot;), las = 2, cex.axis = 1, ) y &lt;- dispersion(&quot;GERMAPARL&quot;, query = &quot;Flüchtlinge&quot;, sAttribute = &quot;year&quot;) cdu &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;CDU&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size gruene &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;GRUENE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size cnt_cdu &lt;- count( cdu, query = c(&quot;Ausländer&quot;, &quot;Zuwanderer&quot;, &quot;Einwanderer&quot;, &quot;Migranten&quot;, &quot;Flüchtlinge&quot;) ) cnt_gruene &lt;- count( gruene, query = c(&quot;Ausländer&quot;, &quot;Zuwanderer&quot;, &quot;Einwanderer&quot;, &quot;Migranten&quot;, &quot;Flüchtlinge&quot;) ) par(mfrow = c(1,2)) barplot( cnt_cdu[[&quot;count&quot;]], names.arg = cnt_cdu[[&quot;query&quot;]], las = 2, ylim = c(0,2000), main = &quot;CDU&quot; ) barplot( cnt_gruene[[&quot;count&quot;]], names.arg = cnt_gruene[[&quot;query&quot;]], las = 2, ylim = c(0,2000), main = &quot;Grüne&quot; ) Now we move to regular expressions. count(&quot;GERMAPARL&quot;, &#39;&quot;Multikult.*&quot;&#39;, cqp = TRUE) ## query count freq ## 1: &quot;Multikult.*&quot; 101 9.998643e-07 count(&quot;GERMAPARL&quot;, &#39;&quot;Multikult.*&quot;&#39;, cqp = TRUE, breakdown = TRUE) ## query match count share ## 1: &quot;Multikult.*&quot; Multikulti 48 47.52 ## 2: &quot;Multikult.*&quot; Multikulturalität 6 5.94 ## 3: &quot;Multikult.*&quot; Multikultiromantik 5 4.95 ## 4: &quot;Multikult.*&quot; Multikultidenken 3 2.97 ## 5: &quot;Multikult.*&quot; Multikultigesellschaft 3 2.97 ## 6: &quot;Multikult.*&quot; Multikulturalismus 3 2.97 ## 7: &quot;Multikult.*&quot; Multikulti-Ideologie 2 1.98 ## 8: &quot;Multikult.*&quot; Multikultibasar 2 1.98 ## 9: &quot;Multikult.*&quot; Multikultiideologie 2 1.98 ## 10: &quot;Multikult.*&quot; Multikultipolitik 2 1.98 ## 11: &quot;Multikult.*&quot; Multikultiseligkeit 2 1.98 ## 12: &quot;Multikult.*&quot; Multikultitruppe 2 1.98 ## 13: &quot;Multikult.*&quot; Multikulti-fantasien 1 0.99 ## 14: &quot;Multikult.*&quot; Multikulti-Gesäusel 1 0.99 ## 15: &quot;Multikult.*&quot; Multikulti-Gout 1 0.99 ## 16: &quot;Multikult.*&quot; Multikulti-Irrtum 1 0.99 ## 17: &quot;Multikult.*&quot; Multikulti-Kuschelpädagogik 1 0.99 ## 18: &quot;Multikult.*&quot; Multikulti-Mannschaft 1 0.99 ## 19: &quot;Multikult.*&quot; Multikulti-Mischmasch 1 0.99 ## 20: &quot;Multikult.*&quot; Multikulti-Spektakel 1 0.99 ## 21: &quot;Multikult.*&quot; Multikulti-Sportclub 1 0.99 ## 22: &quot;Multikult.*&quot; Multikultidelirium 1 0.99 ## 23: &quot;Multikult.*&quot; Multikultiexperimente 1 0.99 ## 24: &quot;Multikult.*&quot; Multikultigläubigen 1 0.99 ## 25: &quot;Multikult.*&quot; Multikultikonzepte 1 0.99 ## 26: &quot;Multikult.*&quot; Multikultimannschaft 1 0.99 ## 27: &quot;Multikult.*&quot; Multikultiseligkeiten 1 0.99 ## 28: &quot;Multikult.*&quot; Multikultisierung 1 0.99 ## 29: &quot;Multikult.*&quot; Multikultiträume 1 0.99 ## 30: &quot;Multikult.*&quot; Multikultiträumereien 1 0.99 ## 31: &quot;Multikult.*&quot; Multikultur 1 0.99 ## 32: &quot;Multikult.*&quot; Multikulturell 1 0.99 ## 33: &quot;Multikult.*&quot; Multikulturhaus 1 0.99 ## query match count share sAttributes(&quot;GERMAPARL&quot;, &quot;party&quot;) ## [1] &quot;CDU&quot; &quot;FDP&quot; &quot;SPD&quot; &quot;CSU&quot; ## [5] &quot;GRUENE&quot; &quot;PDS&quot; &quot;&quot; &quot;parteilos&quot; ## [9] &quot;LINKE&quot; &quot;fraktionslos&quot; csu &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;CSU&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size cdu &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;CDU&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size spd &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;SPD&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size gru &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;GRUENE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size 5.1.2 Multi-word expressions and complex queries count(&quot;GERMAPARL&quot;, query = &#39;&quot;Menschen&quot; &quot;mit&quot; &quot;Migrationshintergrund&quot;&#39;) ## query count freq ## 1: &quot;Menschen&quot; &quot;mit&quot; &quot;Migrationshintergrund&quot; 302 2.989693e-06 cnt &lt;- count(&quot;GERMAPARL&quot;, query = &#39;[pos = &quot;ADJA&quot;] &quot;Integration&quot;&#39;, breakdown = TRUE) head(cnt, 10) ## query match count share ## 1: [pos = &quot;ADJA&quot;] &quot;Integration&quot; europäischen Integration 482 19.55 ## 2: [pos = &quot;ADJA&quot;] &quot;Integration&quot; europäische Integration 421 17.07 ## 3: [pos = &quot;ADJA&quot;] &quot;Integration&quot; erfolgreiche Integration 81 3.28 ## 4: [pos = &quot;ADJA&quot;] &quot;Integration&quot; bessere Integration 70 2.84 ## 5: [pos = &quot;ADJA&quot;] &quot;Integration&quot; soziale Integration 64 2.60 ## 6: [pos = &quot;ADJA&quot;] &quot;Integration&quot; gelungene Integration 47 1.91 ## 7: [pos = &quot;ADJA&quot;] &quot;Integration&quot; sozialen Integration 38 1.54 ## 8: [pos = &quot;ADJA&quot;] &quot;Integration&quot; europäischer Integration 36 1.46 ## 9: [pos = &quot;ADJA&quot;] &quot;Integration&quot; politische Integration 35 1.42 ## 10: [pos = &quot;ADJA&quot;] &quot;Integration&quot; gesellschaftliche Integration 34 1.38 "],
["analysing-neighborhoods.html", "6 Analysing Neighborhoods 6.1 Getting started 6.2 Initialization", " 6 Analysing Neighborhoods 6.1 Getting started The neighborhoods of words, and more generally, of lexical items are interesting for many reason. What patterns occurr in a context of 5, 10 or 15 words to the left and to the right of a word, or the match of a query? “You shall know a word by the neighborhood it keeps” (Firth 1957:11) is a frequently quoted, programmatic statement in linguistics. It has inspired statistical approaches to analyse word contexts. In fact, computing collocations is a core technique in corpus linguistics, and extremely productive to gain insights into to the way language is used in a data-driven manner. Analysing collocations has become an essential tool in lexicography, for instance. There are many reasons why social scientists are fascinated by the idea to adapt working with collocations. Collocations come with the promise that the framing of groups and issues may be explored efficiently and productively. But social scientists usually have different research interests than corpus linguists. Rules of thumb and choices that guide corpus linguists in a reasoned and established manner may deserve reconsideration, as we relocate analysing collocations to another discipline. This concerns word context size and statistical measures, for instance. The most important concern of this chapter on the analysis of word context is that deriving statements about meaning from a statistical identification of cooccurrences requires interpretation. This interpretation cannot be derived from statistical measures, but from reading relevant passages of text the statistical measures indicate as being potentially interesting. Moving from words to numbers is extremely productive to reveal patterns, but the interpretation of patterns will require moving from numbers to word again. Before we look into cooccurrences, a remark on terminology. The same statistical procedures are used to identify collocations and cooccurrences. The statement that a word is a collocate is an observation about a linguistic pattern, the stickiness of one word towards another. But the concern here is not linguistic patterns. Just as social scientists are careful not to confuse correlation and causation, the more technical, and less substantial term ‘cooccurrences’ somewhat pushes us to see that findings about cooccurrences are merely statements about a statistical regularity - in need of further inquiry to obtain substantial statements. 6.2 Initialization library(polmineR) use(&quot;GermaParl&quot;) ## ... resetting CORPUS_REGISTRY environment variable: ## ... setting registry: /Library/Frameworks/R.framework/Versions/3.4/Resources/library/GermaParl/extdata/cwb/registry ## ... unloading rcqp library ## ... reloading rcqp library ## ... ... status: OK In addition to polmineR, we will use the packages, tm, magrittr and wordcloud. if (!&quot;tm&quot; %in% available.packages()[,&quot;Package&quot;]) install.packages(&quot;tm&quot;) if (!&quot;magrittr&quot; %in% available.packages()[,&quot;Package&quot;]) install.packages(&quot;magrittr&quot;) if (!&quot;wordcloud&quot; %in% available.packages()[,&quot;Package&quot;]) install.packages(&quot;wordcloud&quot;) library(tm) library(wordcloud) library(magrittr) We use different context sizes. cooccurrences(&quot;GERMAPARL&quot;, query = &quot;Islam&quot;) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, query = &quot;Islam&quot;, left = 10, right = 10) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, query = &quot;Islam&quot;, left = 15, right = 15) ## ... using polmineR.Rcpp for counting Set the context size to a value (15) for the session. options(&quot;polmineR.left&quot; = 15) options(&quot;polmineR.right&quot; = 15) We also define “junk” vocabulary. junk &lt;- c(tm::stopwords(&quot;de&quot;), c(&quot;.&quot;, &quot;&#39;&#39;&quot;, &quot;,&quot;, &quot;``&quot;, &quot;)&quot;, &quot;(&quot;, &quot;-&quot;, &quot;!&quot;, &quot;[&quot;, &quot;]&quot;), &quot;000&quot;) cooccurrences(&quot;GERMAPARL&quot;, &quot;Muslime&quot;) %&gt;% subset(!word %in% junk) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, &quot;Aussiedler&quot;) %&gt;% subset(!word %in% junk) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, &quot;Flüchtlinge&quot;) %&gt;% subset(!word %in% junk) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, &quot;Asylbewerber&quot;) %&gt;% subset(!word %in% junk) ## ... using polmineR.Rcpp for counting cooccurrences(&quot;GERMAPARL&quot;, &quot;Asylsuchende&quot;) %&gt;% subset(!word %in% junk) ## ... using polmineR.Rcpp for counting A very common visualisation is to use wordclouds. hv &lt;- cooccurrences(&quot;GERMAPARL&quot;, &quot;Heimatvertriebene&quot;) %&gt;% subset(!word %in% junk) %&gt;% subset(rank_ll &lt;= 75) pal &lt;- brewer.pal(8,&quot;Dark2&quot;) wordcloud( words = hv[[&quot;word&quot;]], scale = c(2.5, 0.5), freq = hv[[&quot;ll&quot;]] / 5, color = pal, random.color = TRUE ) However, it is much better to use dotplots. cooccurrences(&quot;GERMAPARL&quot;, &quot;Asylbewerber&quot;) %&gt;% subset(!word %in% junk) %&gt;% dotplot() ## ... using polmineR.Rcpp for counting Very often, we want to work with partitions. So what do we see, when we distinguish parties? cdu &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;CDU&quot;, interjection = &quot;FALSE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size csu &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;CSU&quot;, interjection = &quot;FALSE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size spd &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;SPD&quot;, interjection = &quot;FALSE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size gruene &lt;- partition(&quot;GERMAPARL&quot;, party = &quot;GRUENE&quot;, interjection = &quot;FALSE&quot;) ## ... Setting up partition ## ... type of the corpus is plpr ## ... get encoding: latin1 ## ... get cpos and strucs ## ... get partition size cdu %&gt;% cooccurrences(&quot;Flüchtlinge&quot;) %&gt;% subset(!word %in% junk) %&gt;% dotplot() csu %&gt;% cooccurrences(&quot;Flüchtlinge&quot;) %&gt;% subset(!word %in% junk) %&gt;% dotplot() spd %&gt;% cooccurrences(&quot;Flüchtlinge&quot;) %&gt;% subset(!word %in% junk) %&gt;% dotplot() gruene %&gt;% cooccurrences(&quot;Flüchtlinge&quot;) %&gt;% subset(!word %in% junk) %&gt;% dotplot() "]
]
