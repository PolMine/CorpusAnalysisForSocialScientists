---
title: "Using Corpora in Social Science Research (UCSSR)"
subtitle: 'Basics of Sentiment Analysis'
author: "Andreas Blaette"
date: "25. Juli 2018"
output:
  slidy_presentation:
    footer: Copyright (c) 2018, Andreas Blaette
  beamer_presentation: default
  ioslides_presentation: default
editor_options: 
  chunk_output_type: console
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

## Grundlagen der Sentiment-Analyse

Sentiment-Analysen erfreuen sich großer Beliebtheit. Text Mining-Blogs zeigen in vielen Varianten die Möglichkeiten, die Variation von Bewertungen von Texten mit einem numerischen Indikator zu erfassen und Veränderungen im Zeitverlauf zu analysieren und darzustellen.

Welche Kinofilme werden besonders gut oder besonders schlecht bewertet? Dies lässt sich anhand von Filmbesprechungen untersuchen. Wie ist die Resonanz von Kunden auf ein neu in den Markt eingeführtes Produkt? Dazu lassen sich Kommentare in sozialen Medien untersuchen. Es gibt sicherlich eine Palette nützlicher Einsatzszenarien für Sentiment-Analysen, gerade auch jenseits der Wissenschaft.

Welchen Nutzen haben Sentiment-Analysen in wissenschaftlichen Arbeiten? Zentrale Fragen sind hier, was man eigentlich misst, wenn man "Sentiments" misst. Ist die Validität der Messung gegeben, misst man was man glaubt zu messen? Aus den Antworten leitet sich ab, wann und wie Sentiment-Analysen gut begründet als Forschungsinstrument eingesetzt werden können. Wesentlich ist dabei folgender Unterschied:

  * _Diktionärsbasierte_ Verfahren messen anhand von Listen mit positivem / negativem Vokabular.
  * _Machine Learning_-basierte Verfahren gehen aus von Trainingsdaten mit bekannten Bewertungen und treffen qua Algorithmus Ableitungen für neu zu bewertende Texte. 
  
In dieser Anleitung arbeiten wir mit einem - deutlich einfacheren - diktionärsbasierten Verfahren und einem klassischen Diktionär, "SentiWS".


---- 

## Erforderliche Installationen / Pakete

Die folgenden Erläuterungen nutzen das `polmineR`-Paket und das `GermaParl`-Korpus. Die Installation wird in einem eigenen Foliensatz erläutert. Ergänzend nutzen wir die folgenden Pakete:

  * `zoo`: Ein Paket zur Arbeit mit Zeitreihen-Daten;
  * `magrittr`: Tools, um R-Befehle in einer "Pipe"" hintereinander zu verketten (s.u.);
  * `devtools`: Entwicklertools, wir nutzen einen Befehl für den Download einer einzelnen Funktion;

Der folgende Code prüft, ob diese Pakete installiert sind und nimmt die Installation vor, falls erforderlich.

```{r install_required_packages, eval = TRUE}
required_packages <- c("zoo", "magrittr", "devtools")
for (pkg in required_packages){
  if (pkg %in% rownames(installed.packages()) == FALSE) install.packages(pkg)
}
```

Bitte beachten Sie, dass die Funktionalität für den folgenden Workflow erst mit polmineR-Version `r as.package_version("0.7.9.9005")` zur Verfügung steht. Installieren Sie bei Bedarf die aktuelle Entwicklungsversion von polmineR.

```{r update_polmineR, eval = TRUE}
if (packageVersion("polmineR") < as.package_version("0.7.9.9005"))
  devtools::install_github("PolMine/polmineR", ref = "dev")
```


----


## Los geht's 

Die erforderlichen Pakete werden nun geladen.

```{r load_libraries, eval = TRUE}
library(zoo, quietly = TRUE, warn.conflicts = FALSE)
library(devtools)
library(magrittr)
library(data.table)
```

Wir laden auch polmineR und aktivieren das GermaParl-Korpus, das über das GermaParl-Paket verfügbar ist.

```{r load_polmineR, eval = TRUE}
library(polmineR)
use("GermaParl")
```

----

## Source in function 'get_sentiws' from a GitHub gist

Als Diktionär nutzen wir in der folgenden exemplarischen Analyse den SentimentWortschatz des Leipziger Wortschatz-Projekts, kurz "SentiWS". Eine Erläuterung finden Sie [hier](http://wortschatz.uni-leipzig.de/de/download). Die Daten stehen unter einer CC-BY-SA-NC Lizenz zur Verfügung und können im wissenschaftlichen Kontext ohne lizenzrechtliche Einschränkungen verwendent werden. Erforderlich ist selbstverständlich, dass die Daten korrekt zitiert werden.

SentWS kann als zip-Datei heruntergeladen werden. Um die Dinge weiter zu vereinfachen ist als Gist bei der GitHub-Präsenz des PolMine-Projekts eine Funktion hinterlegt, die den Download erledigt und automatisch eine tabellarische Datenstruktur generiert, wie wir sie benötigen werden.

```{r get_senti_ws, eval = TRUE}
gist_url <- "https://gist.githubusercontent.com/PolMine/70eeb095328070c18bd00ee087272adf/raw/c2eee2f48b11e6d893c19089b444f25b452d2adb/sentiws.R"
devtools::source_url(gist_url) # danach ist Funktion verfügbar
SentiWS <- get_sentiws()
```

Damit ist mit dem Objekt `SentiWS` das Diktionär verfügbar.

----

## SentiWS: Ein Blick in die Daten

Das SentiWS-Objeckt ist ein `data.table`. Wir nutzen das anstelle eines klassischen `data.frame`, weil das später das Matching der Daten (Worte im Korpus und Worte im Diktionär) erleichtert und beschleunigt. Damit wir verstehen, womit wir arbeiten, werfen wir einen schnellen Blick in die Daten.

```{r inspect_senti_ws, eval = TRUE}
head(SentiWS, 5)
```

In der letzten Spalte ("weight") sehen Sie, dass Worten im Diktionär eine Gewichtung zugewiesen ist. Diese kann positiv sein (bei "positivem" Vokabular) oder negativ (bei "negativen" Worten). Außerdem sehen Sie, dass neben der Wortform ("word"-Spalte) auch Lemmata aufgeführt sind sowie eine Part-of-Speech-Klassifikation. Letzteres macht eine eindeutige Zuordnung möglich. Die Spalte "base" mit einem logischen Wert (TRUE/FALSE) ergibt sich technischer aus der Umwandlung der vom Leipziger Wortschatzprojekt heruntergeladenen Tabelle in die extensive Form: In der Roh-Tabelle findet sich in jeder Zeile ein Lemma. Dementsprechend können wir prüfen, wie viele positive bzw. negative Worte als Grundform in der Tabelle sind.

```{r, eval = TRUE}
vocab <- c(positive = nrow(SentiWS[base == TRUE][weight > 0]), negative = nrow(SentiWS[base == TRUE][weight < 0]))
vocab
```

----

# Positives / negatives Vokabular in Wortumfeldern

Wir untersuchen nun das Wortumfeld eines interessanten Begriffs. Weil es so einfach, relevant und plakativ ist, stellen wir die Frage, wie sich die positiven/negativen Konnotationen des Islams im Zeitverlauf entwickelt haben.

Eine Vorfrage ist, wie groß der linke und rechte Wortkontext sein soll, der untersucht wird. In linguistischen Untersuchungen ist ein Kontext von fünf Worten links und rechts gängig. Für politische Bedeutungszuweisungen mag mehr erforderlich sein. Wir gehen von 10 Worten aus und setzen dies folgendermaßen für unsere R-Sitzung fest.


```{r}
options("polmineR.left" = 10L)
options("polmineR.right" = 10L)
```

Über eine "Pipe" generieren wir nun einen `data.frame` ("df") mit den Zählungen des SentiWS-Vokabulars im Wortumfeld von "Islam". Die Pipe ermöglicht es, die Schritte nacheinander durchzuführen, ohne Zwischenergebnisse zu speichern.

```{r}
df <- context("GERMAPARL", query = "Islam", p_attribute = c("word", "pos"), verbose = FALSE) %>%
  partition_bundle(node = FALSE) %>%
  set_names(s_attributes(., s_attribute = "date")) %>%
  weigh(with = SentiWS) %>%
  summary()
```

----

## Die tabellarischen Daten der Sentiment-Analyse

Der df-data.frame führt die Statistik des Wortumfelds eines jeden Auftretens von "Islam" im Korpus auf. Um die Dinge einfach zu halten, arbeiten wir zunächst nicht mit den Gewichtungen, sondern nur mit dem positiven bzw. negativen Worten. Wir vereinfachen die Tabelle entsprechend und sehen sie an.

```{r}
df <- df[, c("name", "size", "positive_n", "negative_n")] 
head(df, n = 15)
```



----

# Aggregation

Als Namen eines Wortkontexts haben wir oben das Datum des Auftretens unseres Suchbegriffs genutzt. Das ermöglicht es, anhand des Datums für das Jahr hochzuaggregieren.

```{r}
df[["year"]] <- as.Date(df[["name"]]) %>% format("%Y-01-01")
df_year <- aggregate(df[,c("size", "positive_n", "negative_n")], list(df[["year"]]), sum)
colnames(df_year)[1] <- "year"
```

Es ist allerdings nicht sinnvoll, mit den absoluten Häufigkeiten zu arbeiten. Daher fügen wir Spalten ein, die den Anteil des negativen bzw. positiven Vokabulars angeben.

```{r}
df_year$negative_share <- df_year$negative_n / df_year$size
df_year$positive_share <- df_year$positive_n / df_year$size
```

Dies wandeln wir in ein Zeitreihen-Objekt im eigentlichen Sinne um.

```{r}
Z <- zoo(
  x = df_year[, c("positive_share", "negative_share")],
  order.by = as.Date(df_year[,"year"])
)
```


----

## Visualisierung

```{r}
plot(
  Z, ylab = "polarity", xlab = "year", main = "Word context of 'Islam': Share of positive/negative vocabulary",
  cex = 0.8, cex.main = 0.8
)
```

----

## Wie gut sind eigentlich die Ergebnisse?

Aber was verbirgt sich eigentlich hinter den numerischen Werten der ermittelten Sentiment-Scores? Um dem nachzugehen, nutzen wir die Möglichkeit von polmineR, eine KWIC-Ausgabe entsprechend einer Positiv-Liste (Vektor mit erforderlichen Worten) zu reduzieren, Worte farblich zu codieren und über Tooltips (hier: Wortgewichte) weitergehende Informationen einzublenden. Also: Fahren Sie auch mit der Maus auf die angemarkerten Worte!

```{r, eval = TRUE}
words_positive <- SentiWS[weight > 0][["word"]]
words_negative <- SentiWS[weight < 0][["word"]]
```

```{r, eval = TRUE}
kwic("GERMAPARL", query = "Islam", positivelist = c(words_positive, words_negative)) %>%
  highlight(lightgreen = words_positive, orange = words_negative) %>%
  tooltips(setNames(SentiWS[["word"]], SentiWS[["weight"]])) %>%
  as("htmlwidget") -> Y
```

Das im Objekt Y gespeicherte Ergebnis (ein 'htmlwidget') sehen wir uns auf einer gesonderten Folie an.

----

```{r, eval = TRUE, echo = FALSE}
Y
```

----

## Diskussion

  * Wir interpretieren Sie die Ergebnisse der Zeitreihen-Analyse?
  * Wie valide sind die Ergebnisse?
  * Ist der Übergang zur Arbeit mit Wort-Gewichten sinnvoll oder erforderlich?